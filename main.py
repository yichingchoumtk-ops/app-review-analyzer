import os
import json
import requests
import pandas as pd
import gspread
from datetime import datetime
from app_store_scraper import AppStore
from google_play_scraper import Sort, reviews
import time

# ===================================================================
# 1. ä» GitHub Secrets è¯»å–å®‰å…¨å‡­è¯
# ===================================================================
print("STEP 1: Reading credentials from GitHub Secrets...")
try:
    dify_api_key = os.environ['DIFY_API_KEY']
    dify_api_url = os.environ['DIFY_API_URL']
    google_creds_json = os.environ['GOOGLE_SHEETS_CREDENTIALS']
    
    # å°† JSON å­—ç¬¦ä¸²è½¬æ¢ä¸º gspread éœ€è¦çš„å­—å…¸æ ¼å¼
    google_creds_dict = json.loads(google_creds_json)
    
    print("âœ… Successfully read all credentials.")
except KeyError as e:
    print(f"âŒ ERROR: A required secret is missing in GitHub Secrets: {e}")
    exit(1) # ä¸¥é‡é”™è¯¯ï¼Œç›´æ¥é€€å‡ºç¨‹åº

# ===================================================================
# 2. è®¾å®š Google Sheets è¿æ¥
# ===================================================================
print("\nSTEP 2: Connecting to Google Sheets...")
try:
    gc = gspread.service_account_from_dict(google_creds_dict)
    
    # !!! å”¯ä¸€éœ€è¦ä½ ç¡®è®¤å’Œä¿®æ”¹çš„åœ°æ–¹ !!!
    # è¯·ç¡®ä¿è¿™é‡Œçš„åç§°ä¸ä½ çš„ Google Sheet æ¡£æ¡ˆåç§°å®Œå…¨ä¸€è‡´
    spreadsheet_name = "App è©•è«–è‡ªå‹•åŒ–æ´å¯Ÿç³»çµ±"
    spreadsheet = gc.open(spreadsheet_name) 
    worksheet_name = "è©•è«–è³‡æ–™åº« (Reviews_DB)"
    worksheet = spreadsheet.worksheet(worksheet_name)
    
    print(f"âœ… Successfully connected to Google Sheet: '{spreadsheet_name}' -> Worksheet: '{worksheet_name}'")
except Exception as e:
    print(f"âŒ ERROR: Failed to connect to Google Sheets: {e}")
    exit(1)

# ===================================================================
# 3. å®šä¹‰ Dify AI åˆ†æåŠŸèƒ½
# ===================================================================
def analyze_with_dify(comment):
    """
    è°ƒç”¨ Dify Workflow API å¯¹å•æ¡è¯„è®ºè¿›è¡Œåˆ†æã€‚
    """
    headers = {
        "Authorization": f"Bearer {dify_api_key}",
        "Content-Type": "application/json",
    }
    payload = {
        "inputs": {
            "review_text": comment # è¿™ä¸ª key å¿…é¡»ä¸ Dify "å¼€å§‹"èŠ‚ç‚¹å®šä¹‰çš„å˜é‡åä¸€è‡´
        },
        "response_mode": "blocking",
        "user": "github-actions-scraper" 
    }
    
    try:
        response = requests.post(dify_api_url, headers=headers, json=payload, timeout=60) # å¢åŠ 60ç§’è¶…æ—¶
        response.raise_for_status() # å¦‚æœè¯·æ±‚å¤±è´¥ (å¦‚ 4xx or 5xx)ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
        
        response_json = response.json()
        
        # Dify å·¥ä½œæµçš„è¾“å‡ºé€šå¸¸åœ¨ 'outputs' é”®ä¸­
        # å‡è®¾æˆ‘ä»¬åœ¨ Dify çš„ç»“æŸèŠ‚ç‚¹å°†è¾“å‡ºå‘½åä¸º 'analysis_result'
        result_text = response_json.get('outputs', {}).get('analysis_result')
        
        if not result_text:
            raise KeyError("'analysis_result' not found in Dify response outputs.")

        return json.loads(result_text)
        
    except requests.exceptions.RequestException as e:
        print(f"  â””â”€ âŒ Dify API call failed: {e}")
    except (KeyError, json.JSONDecodeError) as e:
        print(f"  â””â”€ âŒ Failed to parse Dify response: {e}, Response content: {response.text}")
        
    return None # å¦‚æœä»»ä½•æ­¥éª¤å¤±è´¥ï¼Œåˆ™è¿”å› None

# ===================================================================
# 4. å®šä¹‰çˆ¬è™«åŠŸèƒ½ä¸æ™ºèƒ½ç­›é€‰
# ===================================================================
def get_reviews_and_filter():
    """
    æŠ“å–æ‰€æœ‰ App çš„æœ€æ–°è¯„è®ºï¼Œå¹¶è¿›è¡Œæ™ºèƒ½ç­›é€‰ã€‚
    """
    # åœ¨æœªæ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä» Google Sheet çš„ '40ä¸ªapp' åˆ†é¡µåŠ¨æ€è¯»å–è¿™ä¸ªæ¸…å•
    # ä¸ºäº†ç®€åŒ– Phase 1ï¼Œæˆ‘ä»¬å…ˆå†™æ­»å‡ ä¸ª App è¿›è¡Œæµ‹è¯•
    apps_to_scrape = [
        {'name': 'ä¸‰ç«¹è‚¡å¸‚', 'platform': 'iOS', 'id': '352743563'},
        {'name': 'ä¸‰ç«¹è‚¡å¸‚', 'platform': 'Android', 'id': 'com.mtk'},
        {'name': 'å¯Œæœ Fugle', 'platform': 'iOS', 'id': '1542310263'},
        {'name': 'å¯Œæœ Fugle', 'platform': 'Android', 'id': 'tw.fugle.flutter.app'},
        {'name': 'XQ å…¨çƒè´å®¶', 'platform': 'iOS', 'id': '642738082'},
        {'name': 'XQ å…¨çƒè´å®¶', 'platform': 'Android', 'id': 'djapp.app.xqm'},
    ]
    
    all_new_reviews = []
    
    for app in apps_to_scrape:
        print(f"\nâ–¶ï¸  Fetching reviews for: {app['name']} ({app['platform']})")
        try:
            if app['platform'] == 'iOS':
                scraper = AppStore(country='tw', app_id=app['id'])
                scraper.review(how_many=100) # æŠ“å–æœ€æ–°çš„ 100 ç¬”è¯„è®º
                reviews_list = scraper.reviews
            else: # Android
                reviews_list, _ = reviews(app['id'], lang='zh-TW', country='tw', sort=Sort.NEWEST, count=100)

            for review in reviews_list:
                 all_new_reviews.append({
                    'app_name': app['name'],
                    'platform': app['platform'],
                    'comment': str(review.get('review') or review.get('content', '')), # å…¼å®¹ iOS/Android å¹¶ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                    'rating': int(review.get('rating') or review.get('score', 0)),
                    'date': (review.get('date') or review.get('at')).strftime('%Y-%m-%d %H:%M:%S')
                 })
        except Exception as e:
            print(f"  â””â”€ âš ï¸ Could not fetch reviews for this app: {e}")
             
    if not all_new_reviews:
        print("\nNo new reviews found across all apps.")
        return []

    # --- æ™ºèƒ½ç­›é€‰ ---
    # ä¸ºäº†ä¸è¶…é Dify æ¯æœˆ 200 æ¬¡çš„é™åˆ¶ï¼Œæˆ‘ä»¬æ¯å‘¨çš„ç›®æ ‡æ˜¯åˆ†æçº¦ 40-50 æ¡è¯„è®º
    DIFY_WEEKLY_LIMIT = 40
    
    df = pd.DataFrame(all_new_reviews)
    df.drop_duplicates(subset=['comment'], inplace=True, keep='first') # å»é‡
    
    # ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹© 3 æ˜ŸåŠä»¥ä¸‹çš„ï¼Œå¦‚æœè¿˜ä¸å¤Ÿï¼Œå†ç”¨æœ€æ–°çš„ 4-5 æ˜Ÿè¯„è®ºè¡¥è¶³
    low_rating_reviews = df[df['rating'] <= 3].sort_values(by='date', ascending=False)
    high_rating_reviews = df[df['rating'] > 3].sort_values(by='date', ascending=False)
    
    if len(low_rating_reviews) >= DIFY_WEEKLY_LIMIT:
        reviews_to_analyze = low_rating_reviews.head(DIFY_WEEKLY_LIMIT)
    else:
        needed = DIFY_WEEKLY_LIMIT - len(low_rating_reviews)
        reviews_to_analyze = pd.concat([low_rating_reviews, high_rating_reviews.head(needed)])
        
    print(f"\nâœ… Found {len(df)} unique new reviews. After smart filtering, {len(reviews_to_analyze)} will be sent for AI analysis.")
    return reviews_to_analyze.to_dict('records')

# ===================================================================
# 5. ä¸»æ‰§è¡Œæµç¨‹
# ===================================================================
if __name__ == "__main__":
    
    print("\nSTEP 3: Fetching and filtering reviews...")
    reviews_to_process = get_reviews_and_filter()
    final_results_to_sheet = []
    
    if reviews_to_process:
        print("\nSTEP 4: Sending reviews to Dify for analysis...")
        for i, review in enumerate(reviews_to_process):
            print(f"  - Analyzing review {i+1}/{len(reviews_to_process)} ({review['app_name']}): \"{review['comment'][:40].replace('\n', ' ')}...\"")
            
            # å‘¼å« AI è¿›è¡Œåˆ†æ
            ai_result = analyze_with_dify(review['comment'])
            
            if ai_result:
                print(f"  â””â”€ ğŸ¤– AI Result: {ai_result}")
                # åˆä½µåŸå§‹è¯„è®ºå’Œ AI åˆ†æç»“æœ
                final_row = {
                    'App_åç¨±': review['app_name'],
                    'å¹³å°': review['platform'],
                    'è©•è«–æ—¥æœŸ': review['date'],
                    'åŸå§‹æ˜Ÿç­‰': review['rating'],
                    'è©•è«–å…§å®¹': review['comment'],
                    'AIæƒ…ç·’åˆ†æ•¸': ai_result.get('emotion_score'),
                    'AIåˆ†é¡': ai_result.get('category'),
                    'AIç¸½çµ': ai_result.get('summary'),
                    'è™•ç†æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                final_results_to_sheet.append(final_row)
            else:
                print("  â””â”€ âš ï¸ AI analysis failed for this review, skipping.")
            
            time.sleep(1) # ç¤¼è²Œæ€§åœ°ç­‰å¾…1ç§’ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

    if final_results_to_sheet:
        print(f"\nSTEP 5: Writing {len(final_results_to_sheet)} results to Google Sheets...")
        
        try:
            # æ£€æŸ¥è¡¨å¤´æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–ä¸åŒ¹é…ï¼Œåˆ™å…ˆå†™å…¥è¡¨å¤´
            headers = list(final_results_to_sheet[0].keys())
            try:
                sheet_headers = worksheet.row_values(1)
            except gspread.exceptions.APIError: # å¤„ç†å·¥ä½œè¡¨ä¸ºç©ºçš„æƒ…å†µ
                sheet_headers = []

            if sheet_headers != headers:
                print("  - Writing headers to the sheet...")
                worksheet.update('A1', [headers])
            
            # å°†å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º gspread éœ€è¦çš„æ ¼å¼ (åˆ—è¡¨çš„åˆ—è¡¨)
            values_to_append = [list(row.values()) for row in final_results_to_sheet]
            worksheet.append_rows(values_to_append, value_input_option='USER_ENTERED')
            
            print("âœ… Successfully wrote results to Google Sheets!")
        except Exception as e:
            print(f"âŒ ERROR: An error occurred while writing to Google Sheets: {e}")
            
    print("\nğŸ‰ Workflow finished!")
